## 神经网络

### 背景

人们想要创造出一种可以模仿人类大脑结构的算法。

### 大脑中的神经元

![](http://oss.pyaxy.xyz/img/17.png)

胞体负责接受信号，轴突负责发送信号。

### 神经元模型：Logistic unit

![](http://oss.pyaxy.xyz/img/18.png)

上面为神经网络的一个神经元，相当于神经元接受一些输入，经过处理后再输出计算的结果$h_\theta(x)$，其中$h_\theta(x)$为$\frac{1}{1+e^{-\theta^Tx}}$

![](http://oss.pyaxy.xyz/img/19.png)

一般来说可以将$x_1,x_2,x_3$作为输入数据，我们通常会添加一个$x_0=1$作为偏置单元同样作为输入

刚才描述了一个以$sigmoid$函数作为激活函数($activation function$)的人工神经元，代表一个神经元，神经网络实际上就是很多个神经元组合起来的模型：![](http://oss.pyaxy.xyz/img/20.png)

第一层称为输入层，最后一层输出层，其他层都称为隐藏层。

上图中的$a_1^{(2)}$表示第2层的第一个激活项(activation)

$\Theta^{j}$表示控制第j层到j+1层的映射的权重矩阵，比如第1层经过$\Theta^1$的处理后才变成了第2层的数据。

![](http://oss.pyaxy.xyz/img/21.png)

这边是具体的权重矩阵与激活项的计算细节。

我们定义上面的$\Theta^{(1)}_{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(1)}_{13}x_3=z^{(2)}_1$

即有：
$$
a^{(2)}_1=g(z^{(2)}_1)
$$

$$
x=\begin{bmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{bmatrix}
&&&&z^{(2)}=\begin{bmatrix}
z^{(2)}_1\\
z^{(2)}_2\\
z^{(2)}_3
\end{bmatrix}
$$

$$
z^{(2)}=\Theta^{(1)}x
$$

### 举例

假设我们有两个输入值：

![](http://oss.pyaxy.xyz/img/22.png)

表示只有当$x_1,x_2$都为1或0时结果为假，现在我们需要一个模型来学习它。

我们先介绍逻辑**与**的神经网络结构：

![](http://oss.pyaxy.xyz/img/23.png)

上图中的$+1$为偏置单元，我们得到的$\Theta$为：-30，20，20，这样就有了
$$
h_\Theta(x)=g(-30+20x_1+20x_2)
$$
这样就简单表示了**与**运算。可以自己画一个真值表来验证。

我们再来接受逻辑**或**的神经网络结构：

![](http://oss.pyaxy.xyz/img/24.png)

得到的$\Theta$为-10，20，20

逻辑**非**的结构;

![](http://oss.pyaxy.xyz/img/25.png)

现在我们把这三个结构结合起来：

![](http://oss.pyaxy.xyz/img/26.png)

其中红色的节点表示$x_1,x_2$都为1时，蓝色表示$x_1,x_2$都为0时，绿色的OR运算表示红色和蓝色成立一种时就输入肯定答案。

这样神经网络的作用就体现出来了，神经网络可以将复杂的算法简单化，把复杂的算法在每一层用简单的算法表示出来。

### 多分类问题

对于多分类问题，我们可以让$h_\theta(x)$的输出为矩阵。比如$\begin{bmatrix}1\\0\\0\end{bmatrix}$表示第一种结果，$\begin{bmatrix}0\\1\\0\end{bmatrix}$表示第二种结果，$\begin{bmatrix}0\\0\\1\end{bmatrix}$表示第三种结果。

### 代价函数

定义：

$L$为神经网络的层数

$s_l$为第l层神经网络的结点数量，特别定义$K$为输出层的结点数量

**神经网络的代价函数**：
$$
J(\Theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$
前面的$k$的和表示多分类时，多个输出结点的输出结果之和，当$K=1$时可忽略。

后半项的作用为正则化，其中不正则化下标为$0$的$\Theta$，这点于$logistic$回归一致。

### **反向传播算法**

回顾我们的神经网络的代价函数，我们的目标是最小化$J(\Theta)$，我们需要计算：

* $J(\Theta)$
* $\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$

我们定义：$\delta_j^{(l)}$表示第$l$层第$j$个结点的误差。

举个例子(layer L = 4)：

$\delta^{(4)}=a^{(4)}-y$

$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}·*g'(z^{(3)})$

$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}·*g'(z^{(2)})$

其中$g'(z)=a·*(1-a)$，计算的时候只需修改上标即可，比如$g'(z^{(2)})=a^{(2)}·*(1-a^{(2)})$

如果我们不考虑正则化的话，可以证明：
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}
$$
算法步骤：

对于$m$个训练集

初始化$\triangle_{ij}^{(l)}=0$

对每个训练集$i=1$ to $m$：

​	set $a^{(1)}=x^{(i)}$

​	正向传播计算出每一层的值

​	使用$y^{(i)}$来计算$\delta^{(L)}=a^{(L)}-y^{(i)}$

​	反向传播计算出每一个$\delta$

​	累加：$\triangle_{ij}^{(l)}:=\triangle_{ij}^{(l)}+a_j^{(l)}\delta_{i}^{(l+1)}$

$D_{ij}^{(l)}:=\frac{1}{m}\triangle_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}\quad if \quad j\neq0$

$D_{ij}^{(l)}:=\frac{1}{m}\triangle_{ij}^{(l)}\quad if \quad j=0$

可以证明：
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}
$$

### 理解反向传播

![](http://oss.pyaxy.xyz/img/27.png)

### 小细节：展开参数（矩阵到向量）

在我们求$minJ(\Theta)$的过程中，我们通常会用到matlab中更为高级的优化算法，但是在算法的接口中我们只有一个$\Theta$参数的传入，神经网络中我们却有多个$\Theta^{(l)}$，因此我们需要将这些$\Theta$展开为一个向量传入算法中。

举个例子：

![](http://oss.pyaxy.xyz/img/28.png)

把矩阵展开为一个向量：

```matlab
thetaVec = [ Theta1(:); Theta2(:); Theta3(:) ];
DVec = [D1(:); D2(:); D3(:)];
```

恢复向量到矩阵：

```matlab
Theta1 = reshape(thetaVec(1:110),10,11);
Theta2 = reshape(thetaVec(111:220),10,11);
Theta3 = reshape(thetaVec(221:231),1,11);
```

### 梯度检测

反向传播的实现中，经常会出现一些bug，因此我们需要一种检验方式来检测反向传播是否正常运作。

![](http://oss.pyaxy.xyz/img/29.png)

我们使用偏导数的定义来求取梯度，与反向传播的对比，如果一致则继续使用反向传播，**记住一定要取消梯度检测的代码，梯度检测的时间复杂度远高于反向传播。**

### 随机初始化

在前面的线性回归和逻辑回归中，我们通常为$\Theta$取初始值为0，但是在反向传播中，我们不能将值取为0。

在反向传播中，$\Theta$表示权重，在反向传播的过程中，如果所有的权重一致，则结点的计算值就会一致，会造成大量的冗余。

因此我们需要使用随机数来初始化$\Theta$：

```matlab
Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSiLON;
```

这样会使$\Theta$的值处于$[-\epsilon,+\epsilon]$之间

### 总结

#### 选择神经网络结构

输入层：数量为特征的种类数

输出层：分类数

较为理想的结构：1层隐藏层，如果大于一层，最好隐藏层的结点数都相等。（一般来说层数越多越好，但是学习所耗费的时间更多）

#### 训练神经网络

1. 随机初始化权重
2. 正向传播计算出每个结点的值
3. 计算$J(\Theta)$
4. 反向传播计算梯度
5. 使用梯度检测来检测反向传播是否正确
6. 使用高级优化算法最小化$J(\Theta)$## 神经网络

### 背景

人们想要创造出一种可以模仿人类大脑结构的算法。

### 大脑中的神经元

![](http://oss.pyaxy.xyz/img/17.png)

胞体负责接受信号，轴突负责发送信号。

### 神经元模型：Logistic unit

![](http://oss.pyaxy.xyz/img/18.png)

上面为神经网络的一个神经元，相当于神经元接受一些输入，经过处理后再输出计算的结果$h_\theta(x)$，其中$h_\theta(x)$为$\frac{1}{1+e^{-\theta^Tx}}$

![](http://oss.pyaxy.xyz/img/19.png)

一般来说可以将$x_1,x_2,x_3$作为输入数据，我们通常会添加一个$x_0=1$作为偏置单元同样作为输入

刚才描述了一个以$sigmoid$函数作为激活函数($activation function$)的人工神经元，代表一个神经元，神经网络实际上就是很多个神经元组合起来的模型：![](http://oss.pyaxy.xyz/img/20.png)

第一层称为输入层，最后一层输出层，其他层都称为隐藏层。

上图中的$a_1^{(2)}$表示第2层的第一个激活项(activation)

$\Theta^{j}$表示控制第j层到j+1层的映射的权重矩阵，比如第1层经过$\Theta^1$的处理后才变成了第2层的数据。

![](http://oss.pyaxy.xyz/img/21.png)

这边是具体的权重矩阵与激活项的计算细节。

我们定义上面的$\Theta^{(1)}_{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(1)}_{13}x_3=z^{(2)}_1$

即有：
$$
a^{(2)}_1=g(z^{(2)}_1)
$$

$$
x=\begin{bmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{bmatrix}
&&&&z^{(2)}=\begin{bmatrix}
z^{(2)}_1\\
z^{(2)}_2\\
z^{(2)}_3
\end{bmatrix}
$$

$$
z^{(2)}=\Theta^{(1)}x
$$

### 举例

假设我们有两个输入值：

![](http://oss.pyaxy.xyz/img/22.png)

表示只有当$x_1,x_2$都为1或0时结果为假，现在我们需要一个模型来学习它。

我们先介绍逻辑**与**的神经网络结构：

![](http://oss.pyaxy.xyz/img/23.png)

上图中的$+1$为偏置单元，我们得到的$\Theta$为：-30，20，20，这样就有了
$$
h_\Theta(x)=g(-30+20x_1+20x_2)
$$
这样就简单表示了**与**运算。可以自己画一个真值表来验证。

我们再来接受逻辑**或**的神经网络结构：

![](http://oss.pyaxy.xyz/img/24.png)

得到的$\Theta$为-10，20，20

逻辑**非**的结构;

![](http://oss.pyaxy.xyz/img/25.png)

现在我们把这三个结构结合起来：

![](http://oss.pyaxy.xyz/img/26.png)

其中红色的节点表示$x_1,x_2$都为1时，蓝色表示$x_1,x_2$都为0时，绿色的OR运算表示红色和蓝色成立一种时就输入肯定答案。

这样神经网络的作用就体现出来了，神经网络可以将复杂的算法简单化，把复杂的算法在每一层用简单的算法表示出来。

### 多分类问题

对于多分类问题，我们可以让$h_\theta(x)$的输出为矩阵。比如$\begin{bmatrix}1\\0\\0\end{bmatrix}$表示第一种结果，$\begin{bmatrix}0\\1\\0\end{bmatrix}$表示第二种结果，$\begin{bmatrix}0\\0\\1\end{bmatrix}$表示第三种结果。

### 代价函数

定义：

$L$为神经网络的层数

$s_l$为第l层神经网络的结点数量，特别定义$K$为输出层的结点数量

**神经网络的代价函数**：
$$
J(\Theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$
前面的$k$的和表示多分类时，多个输出结点的输出结果之和，当$K=1$时可忽略。

后半项的作用为正则化，其中不正则化下标为$0$的$\Theta$，这点于$logistic$回归一致。

### **反向传播算法**

回顾我们的神经网络的代价函数，我们的目标是最小化$J(\Theta)$，我们需要计算：

* $J(\Theta)$
* $\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$

我们定义：$\delta_j^{(l)}$表示第$l$层第$j$个结点的误差。

举个例子(layer L = 4)：

$\delta^{(4)}=a^{(4)}-y$

$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}·*g'(z^{(3)})$

$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}·*g'(z^{(2)})$

其中$g'(z)=a·*(1-a)$，计算的时候只需修改上标即可，比如$g'(z^{(2)})=a^{(2)}·*(1-a^{(2)})$

如果我们不考虑正则化的话，可以证明：
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}
$$
算法步骤：

对于$m$个训练集

初始化$\triangle_{ij}^{(l)}=0$

对每个训练集$i=1$ to $m$：

​	set $a^{(1)}=x^{(i)}$

​	正向传播计算出每一层的值

​	使用$y^{(i)}$来计算$\delta^{(L)}=a^{(L)}-y^{(i)}$

​	反向传播计算出每一个$\delta$

​	累加：$\triangle_{ij}^{(l)}:=\triangle_{ij}^{(l)}+a_j^{(l)}\delta_{i}^{(l+1)}$

$D_{ij}^{(l)}:=\frac{1}{m}\triangle_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}\quad if \quad j\neq0$

$D_{ij}^{(l)}:=\frac{1}{m}\triangle_{ij}^{(l)}\quad if \quad j=0$

可以证明：
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}
$$

### 理解反向传播

![](http://oss.pyaxy.xyz/img/27.png)

### 小细节：展开参数（矩阵到向量）

在我们求$minJ(\Theta)$的过程中，我们通常会用到matlab中更为高级的优化算法，但是在算法的接口中我们只有一个$\Theta$参数的传入，神经网络中我们却有多个$\Theta^{(l)}$，因此我们需要将这些$\Theta$展开为一个向量传入算法中。

举个例子：

![](http://oss.pyaxy.xyz/img/28.png)

把矩阵展开为一个向量：

```matlab
thetaVec = [ Theta1(:); Theta2(:); Theta3(:) ];
DVec = [D1(:); D2(:); D3(:)];
```

恢复向量到矩阵：

```matlab
Theta1 = reshape(thetaVec(1:110),10,11);
Theta2 = reshape(thetaVec(111:220),10,11);
Theta3 = reshape(thetaVec(221:231),1,11);
```

### 梯度检测

反向传播的实现中，经常会出现一些bug，因此我们需要一种检验方式来检测反向传播是否正常运作。

![](http://oss.pyaxy.xyz/img/29.png)

我们使用偏导数的定义来求取梯度，与反向传播的对比，如果一致则继续使用反向传播，**记住一定要取消梯度检测的代码，梯度检测的时间复杂度远高于反向传播。**

### 随机初始化

在前面的线性回归和逻辑回归中，我们通常为$\Theta$取初始值为0，但是在反向传播中，我们不能将值取为0。

在反向传播中，$\Theta$表示权重，在反向传播的过程中，如果所有的权重一致，则结点的计算值就会一致，会造成大量的冗余。

因此我们需要使用随机数来初始化$\Theta$：

```matlab
Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSiLON;
```

这样会使$\Theta$的值处于$[-\epsilon,+\epsilon]$之间

### 总结

#### 选择神经网络结构

输入层：数量为特征的种类数

输出层：分类数

较为理想的结构：1层隐藏层，如果大于一层，最好隐藏层的结点数都相等。（一般来说层数越多越好，但是学习所耗费的时间更多）

#### 训练神经网络

1. 随机初始化权重
2. 正向传播计算出每个结点的值
3. 计算$J(\Theta)$
4. 反向传播计算梯度
5. 使用梯度检测来检测反向传播是否正确
6. 使用高级优化算法最小化$J(\Theta)$