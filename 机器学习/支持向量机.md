## 支持向量机

### 优化目标

回忆我们之前学习的逻辑回归，我们的$h_\theta(x)$为：
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
如果$y=1$，我们希望预测的值接近于$1$，从$sigmoid$函数图像来看，这要求$\theta^Tx$远远大于$0$。

再去看逻辑回归的代价函数，每一个样本对代价函数的代价如图：

![](http://oss.pyaxy.xyz/img/38.png)

为了构建支持向量机，我们对上述图像做出一点修改：

![](http://oss.pyaxy.xyz/img/39.png)

让代价函数在$z>1$的地方等于$0$，在$z<1$的地方近似地描绘出一条和逻辑回归曲线相似的直线。我们称这个代价函数为$cost_1(z)$

同理，有：

![](http://oss.pyaxy.xyz/img/40.png)

经过修改之后的支持向量机的优化目标为：
$$
\min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta^2_j
$$
上述的$C$与逻辑回归中的正则化的$\lambda$一样，用于控制前后的权重以此来控制算法的方差与偏差。

最后，与逻辑回归不同的是，SVM不会输出概率，会直接输出结果，如果$\theta^Tx>0$则直接输出$1$。

### 直观上对大间隔的理解

SVM的决策边界如图：

![](http://oss.pyaxy.xyz/img/41.png)

图中黑色的线为其决策边界，其与其他样本的最小距离更大一些，具有更好的鲁棒性。

### 大间隔的数学原理

以有两个特征量的模型举例，我们的优化目标为：
$$
\min_\theta \frac{1}{2}\sum_{j=1}^n\theta^2_j 
$$
与之前讲的优化目标不一样的是前面的式子消失了，这是因为我们将$C$设置的非常大，当$\theta^Tx>1$时$cost(\theta^Tx)$即为零，所以只剩下了后一项。根据我们高中的知识，上述的式子又可以化为：
$$
\min_\theta \frac{1}{2}\Vert\theta\Vert^2
$$
上述表示$\theta$向量的模的平方。

在我们优化的过程中，我们需要使$\theta^Tx>1\\\theta^T x<-1$，这也可以化简为$p^{(i)}\cdot\Vert\theta\Vert>1\\p^{(i)}\cdot\Vert\theta\Vert<-1\\$，其中$p^{(i)}$表示$x^{(i)}$在$\theta$上的投影。

分析上式，我们的目标是最小化$\Vert\theta\Vert$同时使$p^{(i)}\cdot\Vert\theta\Vert>1\\p^{(i)}\cdot\Vert\theta\Vert<-1\\$，因此当$\vert p^{(i)}\vert$越大，对$\theta$要求越小。

![](http://oss.pyaxy.xyz/img/42.png)

上图中的绿色为决策边界，$\theta$与其正交(因为$\theta^T*x=0$)，从其样本点在$\theta$上的投影为红色区域，显然比较小，因此不能得到一个很好的优化效果。

同理：

![](http://oss.pyaxy.xyz/img/43.png)

上述的图像可以使投影长度最大，从而达到最小$\Vert\theta\Vert$，绿线为决策边界，拥有着较大的间隔。



### 核函数

![](http://oss.pyaxy.xyz/img/44.png)

在上面非线性模型中，我们想要拟合它的边界曲线，我们一般是使用高阶多项式来拟合这个模型，不过使用高阶多项式的问题也显而易见，我们并不知道什么样的高阶多项式是对我们的算法是有效的，高级多项式的选择成为了一个问题，而核函数的出现则代替的高阶多项式来表示模型。

![](http://oss.pyaxy.xyz/img/45.png)

在给定的样本空间内，我们手动选择了三个点，同时我们定义：
$$
f_1 = similarity(x,l^{(1)})=exp(-\frac{\Vert x-l^{(1)}\Vert}{2\sigma^2})\\
f_2 = similarity(x,l^{(2)})=exp(-\frac{\Vert x-l^{(2)}\Vert}{2\sigma^2})\\
f_3 = similarity(x,l^{(3)})=exp(-\frac{\Vert x-l^{(3)}\Vert}{2\sigma^2})
$$
上述函数称为相似度函数，后面的一项为高斯核函数，我们也有其他的相似度核函数，只不过在这里我们使用的是高斯核函数。

#### 核函数的作用：

让我们分析上述的核函数，当我们选择的样本点$x$接近于$l^{(1)}$点时，两点之间的距离近似于$0$，则指数项接近于$0$，因此$f_1=1$，当$x$远离$l^{(1)}$点时，$f_1=0$

![](http://oss.pyaxy.xyz/img/46.png)

上图表现了核函数的图像化特征以及改变$\sigma$对图像的影响。

#### 如何选择标记点

**我们的思想是，将每一个样本点都设定为一个标记点。**然后计算出每一个样本点相对于其他标记点的相似度函数并形成一个向量，这个向量即为我们的特征向量，记为$f^{(i)}$

同时将我们的支持向量机于核函数结合起来得到新的优化函数：
$$
\min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta^2_j
$$

#### $C$与方差和偏差

$C=\frac{1}{\lambda}$，大的$C$对应着更低的偏差，更高的方差。小的$C$对应着更高的偏差，更低的方差。

#### $\sigma$与方差和偏差

大的$\sigma$对应着$f^{(i)}$更为平缓的变化，有着高偏差，低方差。

小的$\sigma$对应着低偏差，高方差。



### 使用SVM

使用封装的软件包来求出$\theta$

自己应该做的事情：

* 拟定$C$
* 选择核函数(如果选择高斯核函数则需要再选择$\sigma$)

线性核函数：
$$
Predict\ ''y=1''\  if\ \theta^Tx>0
$$
注意：在使用高斯核函数之前需要对特征量进行数量级的统一缩放。

#### 多分类问题

许多SVM包已经提供了关于多分类问题的函数，但是如果没有的话，我们仍然会采取之前一对多的思想，将某类单独分出来，其他的类统一归为一类，如此往复，得到一系列$\theta$。

#### 逻辑回归与SVM的选择

设$n=特征量$，$m=样本数量$

当$n$远大于$m$时，使用逻辑回归或者带有线性核函数的SVM

当$n$较小，$m$较大时，我们使用带有高斯核函数的SVM

当$n$较小，$m$极大时，我们需要创建额外的特征，然后使用逻辑回归或者无核SVM

另外，使用SVM不必担心局部最优的问题，它拟合出来的模型一定是全局最优的。## 支持向量机

### 优化目标

回忆我们之前学习的逻辑回归，我们的$h_\theta(x)$为：
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
如果$y=1$，我们希望预测的值接近于$1$，从$sigmoid$函数图像来看，这要求$\theta^Tx$远远大于$0$。

再去看逻辑回归的代价函数，每一个样本对代价函数的代价如图：

![](http://oss.pyaxy.xyz/img/38.png)

为了构建支持向量机，我们对上述图像做出一点修改：

![](http://oss.pyaxy.xyz/img/39.png)

让代价函数在$z>1$的地方等于$0$，在$z<1$的地方近似地描绘出一条和逻辑回归曲线相似的直线。我们称这个代价函数为$cost_1(z)$

同理，有：

![](http://oss.pyaxy.xyz/img/40.png)

经过修改之后的支持向量机的优化目标为：
$$
\min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta^2_j
$$
上述的$C$与逻辑回归中的正则化的$\lambda$一样，用于控制前后的权重以此来控制算法的方差与偏差。

最后，与逻辑回归不同的是，SVM不会输出概率，会直接输出结果，如果$\theta^Tx>0$则直接输出$1$。

### 直观上对大间隔的理解

SVM的决策边界如图：

![](http://oss.pyaxy.xyz/img/41.png)

图中黑色的线为其决策边界，其与其他样本的最小距离更大一些，具有更好的鲁棒性。

### 大间隔的数学原理

以有两个特征量的模型举例，我们的优化目标为：
$$
\min_\theta \frac{1}{2}\sum_{j=1}^n\theta^2_j 
$$
与之前讲的优化目标不一样的是前面的式子消失了，这是因为我们将$C$设置的非常大，当$\theta^Tx>1$时$cost(\theta^Tx)$即为零，所以只剩下了后一项。根据我们高中的知识，上述的式子又可以化为：
$$
\min_\theta \frac{1}{2}\Vert\theta\Vert^2
$$
上述表示$\theta$向量的模的平方。

在我们优化的过程中，我们需要使$\theta^Tx>1\\\theta^T x<-1$，这也可以化简为$p^{(i)}\cdot\Vert\theta\Vert>1\\p^{(i)}\cdot\Vert\theta\Vert<-1\\$，其中$p^{(i)}$表示$x^{(i)}$在$\theta$上的投影。

分析上式，我们的目标是最小化$\Vert\theta\Vert$同时使$p^{(i)}\cdot\Vert\theta\Vert>1\\p^{(i)}\cdot\Vert\theta\Vert<-1\\$，因此当$\vert p^{(i)}\vert$越大，对$\theta$要求越小。

![](http://oss.pyaxy.xyz/img/42.png)

上图中的绿色为决策边界，$\theta$与其正交(因为$\theta^T*x=0$)，从其样本点在$\theta$上的投影为红色区域，显然比较小，因此不能得到一个很好的优化效果。

同理：

![](http://oss.pyaxy.xyz/img/43.png)

上述的图像可以使投影长度最大，从而达到最小$\Vert\theta\Vert$，绿线为决策边界，拥有着较大的间隔。



### 核函数

![](http://oss.pyaxy.xyz/img/44.png)

在上面非线性模型中，我们想要拟合它的边界曲线，我们一般是使用高阶多项式来拟合这个模型，不过使用高阶多项式的问题也显而易见，我们并不知道什么样的高阶多项式是对我们的算法是有效的，高级多项式的选择成为了一个问题，而核函数的出现则代替的高阶多项式来表示模型。

![](http://oss.pyaxy.xyz/img/45.png)

在给定的样本空间内，我们手动选择了三个点，同时我们定义：
$$
f_1 = similarity(x,l^{(1)})=exp(-\frac{\Vert x-l^{(1)}\Vert}{2\sigma^2})\\
f_2 = similarity(x,l^{(2)})=exp(-\frac{\Vert x-l^{(2)}\Vert}{2\sigma^2})\\
f_3 = similarity(x,l^{(3)})=exp(-\frac{\Vert x-l^{(3)}\Vert}{2\sigma^2})
$$
上述函数称为相似度函数，后面的一项为高斯核函数，我们也有其他的相似度核函数，只不过在这里我们使用的是高斯核函数。

#### 核函数的作用：

让我们分析上述的核函数，当我们选择的样本点$x$接近于$l^{(1)}$点时，两点之间的距离近似于$0$，则指数项接近于$0$，因此$f_1=1$，当$x$远离$l^{(1)}$点时，$f_1=0$

![](http://oss.pyaxy.xyz/img/46.png)

上图表现了核函数的图像化特征以及改变$\sigma$对图像的影响。

#### 如何选择标记点

**我们的思想是，将每一个样本点都设定为一个标记点。**然后计算出每一个样本点相对于其他标记点的相似度函数并形成一个向量，这个向量即为我们的特征向量，记为$f^{(i)}$

同时将我们的支持向量机于核函数结合起来得到新的优化函数：
$$
\min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta^2_j
$$

#### $C$与方差和偏差

$C=\frac{1}{\lambda}$，大的$C$对应着更低的偏差，更高的方差。小的$C$对应着更高的偏差，更低的方差。

#### $\sigma$与方差和偏差

大的$\sigma$对应着$f^{(i)}$更为平缓的变化，有着高偏差，低方差。

小的$\sigma$对应着低偏差，高方差。



### 使用SVM

使用封装的软件包来求出$\theta$

自己应该做的事情：

* 拟定$C$
* 选择核函数(如果选择高斯核函数则需要再选择$\sigma$)

线性核函数：
$$
Predict\ ''y=1''\  if\ \theta^Tx>0
$$
注意：在使用高斯核函数之前需要对特征量进行数量级的统一缩放。

#### 多分类问题

许多SVM包已经提供了关于多分类问题的函数，但是如果没有的话，我们仍然会采取之前一对多的思想，将某类单独分出来，其他的类统一归为一类，如此往复，得到一系列$\theta$。

#### 逻辑回归与SVM的选择

设$n=特征量$，$m=样本数量$

当$n$远大于$m$时，使用逻辑回归或者带有线性核函数的SVM

当$n$较小，$m$较大时，我们使用带有高斯核函数的SVM

当$n$较小，$m$极大时，我们需要创建额外的特征，然后使用逻辑回归或者无核SVM

另外，使用SVM不必担心局部最优的问题，它拟合出来的模型一定是全局最优的。