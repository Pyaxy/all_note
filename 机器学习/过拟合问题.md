## 过拟合问题

### 过拟合

过拟合($overfitting$)：也称为过学习，他的直观表现是在学习出的模型在训练集上表现良好，但在实际问题出表现不好，泛化性差。

![](http://oss.pyaxy.xyz/img/15.png)

上图表现出的算法在以给定的训练集上表现良好，但是在实际问题中不能很好的应用，泛化性差，这种问题我们称之为过拟合(通过样本点且波动程度太大)。

一般过拟合会在变量过多的时候出现。

### 如何解决过拟合

1. 尽量减少所选的变量的数量。（缺点是可能舍去与预测有关的变量）
2. 正则化：保持变量数量不变，把$\theta_j$减小。



### 变化代价函数

减小$\theta_j$意味着得到的模型更简单，泛化性更强。

所以在代价函数后面加上以下公式即可减小$\theta_j$
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$
其中$\lambda$为正则化系数，负责控制$\theta$的变化大小。



### 线性回归的正则化

上面我们提到正则化就是修改**代价函数**使之对$\theta_j$进行减小以是模型更为简单。修改之后的代价函数为：
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$


下面是线性回归的正则化

重复：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$
其中对$\theta_0$不进行修改是约定俗成的规矩，对$\theta_0$是否进行正则化对结果的影响不是很大。其中上面对$\theta_j$的梯度下降可写为：
$$
\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$


其中$(1-\alpha\frac{\lambda}{m})$是一个非常接近但小于1的数字。

正规方程的正则化：
$$
\theta=(X^TX+\lambda
\begin{bmatrix}
{0}&&&\\
&{1}&&\\
&&{1}&\\
&&&{1}\\
&&&&{\ddots}\\
&&&&&{1}
\end{bmatrix})^{-1}X^Ty
$$


$logistic$的正则化
$$
J(\theta)=[-\frac{1}{m}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$


重复：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$



## 过拟合问题

### 过拟合

过拟合($overfitting$)：也称为过学习，他的直观表现是在学习出的模型在训练集上表现良好，但在实际问题出表现不好，泛化性差。

![](http://oss.pyaxy.xyz/img/15.png)

上图表现出的算法在以给定的训练集上表现良好，但是在实际问题中不能很好的应用，泛化性差，这种问题我们称之为过拟合(通过样本点且波动程度太大)。

一般过拟合会在变量过多的时候出现。

### 如何解决过拟合

1. 尽量减少所选的变量的数量。（缺点是可能舍去与预测有关的变量）
2. 正则化：保持变量数量不变，把$\theta_j$减小。



### 变化代价函数

减小$\theta_j$意味着得到的模型更简单，泛化性更强。

所以在代价函数后面加上以下公式即可减小$\theta_j$
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$
其中$\lambda$为正则化系数，负责控制$\theta$的变化大小。



### 线性回归的正则化

上面我们提到正则化就是修改**代价函数**使之对$\theta_j$进行减小以是模型更为简单。修改之后的代价函数为：
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$


下面是线性回归的正则化

重复：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$
其中对$\theta_0$不进行修改是约定俗成的规矩，对$\theta_0$是否进行正则化对结果的影响不是很大。其中上面对$\theta_j$的梯度下降可写为：
$$
\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$


其中$(1-\alpha\frac{\lambda}{m})$是一个非常接近但小于1的数字。

正规方程的正则化：
$$
\theta=(X^TX+\lambda
\begin{bmatrix}
{0}&&&\\
&{1}&&\\
&&{1}&\\
&&&{1}\\
&&&&{\ddots}\\
&&&&&{1}
\end{bmatrix})^{-1}X^Ty
$$


$logistic$的正则化
$$
J(\theta)=[-\frac{1}{m}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$


重复：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$